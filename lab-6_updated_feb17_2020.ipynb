{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS342 Machine Learning\n",
    "## Lab 6: Autoencoders and denoising autoencoders\n",
    "\n",
    "### Department of Computer Science, University of Warwick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will create an autoencoder using the MNIST dataset, a widely-used dataset of hand-written digits. We will be working with the ```mnist_train_x.npy``` and ```mnist_test_x.npy``` data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets for this lab are larger than those for previous labs. Therefore, we will load the data directly from the module directory, as shown below. If you would like to work on the lab using your own machine, copy the data across by running the following command in a terminal window.\n",
    "\n",
    "```scp USERNAME@login-3.dcs.warwick.ac.uk:/modules/cs342/2019/lab6/data/* .```\n",
    "\n",
    "After entering your DCS password, this will copy the data to your current working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The MNIST dataset consists of 60,000 training greyscale images and 5,000 test greyscale images of 28 by 28 pixels hand-written digits. As autoencoders are an unsupervised learning technique, in part 1 we will be working with the training set only. The images in both training and test sets are represented as integers between 0 and 255. *__Please note__: The training and test split used in this lab is not the standardized split you may find in other copies of the dataset online.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST train set shape:  (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import confusion_matrix as c_matrix\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "data_path = '/modules/cs342/2019/lab6/data/'\n",
    "mnist_train_x = np.load(data_path + 'mnist_train_x.npy')\n",
    "print('MNIST train set shape: ', mnist_train_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Display images from the MNIST dataset (5 marks)\n",
    "Create a function, ```display_mnist()```, to display the first 5 images from the ```mnist_train_x.npy``` dataset side-by-side. Ensure the figure is displayed at an appropriate size in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mnist(data):\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    for i in range(5):\n",
    "        axe = fig.add_subplot(1,5,i+1)\n",
    "        plt.imshow(data[i].reshape(28,28), cmap = 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAACUCAYAAACa7UEyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAXp0lEQVR4nO3de5AU5bnH8ecBxKCgslABFHKWS4ghqQRSG1xLSkBQOQTjJRViUAIFiVbqmGBpDKiRixaBFFXRxEviEiKgmCMWJhIhWoCApaIRIodLrD1cDEZFOBRyUYNA+Z4/GM/Z5+3dmZ6d7pnu6e+naor99c50v7vz2LwOT7+tzjkBAAAAsqBNpQcAAAAAlAuTXwAAAGQGk18AAABkBpNfAAAAZAaTXwAAAGQGk18AAABkRkmTX1UdpaqNqrpTVadFNShUH2oFYVAnCItaQRjUCZrlnGvVQ0TaisguEekjIu1F5L9EZECB1zge1fOIq1Yq/XPxiPzxP5xTeIR5cE7hEfLBOYVHqEdL73Mpn/wOFpGdzrndzrnjIvKfInJlCftD9aJWsm1PyOdRJwiLWsk2zikoSSmT3/NE5J9N8tu5bYaq3qCqG1V1YwnHQroVrBXqBMI5BeFxTkEYnFPQrHZxH8A51yAiDSIiquriPh7SiTpBWNQKwqBOEBa1kj2lfPL7joj0apJ75rYBPmoFYVAnCItaQRjUCZpVyuT3NRH5vKr2VtX2InKtiCyPZlioMtQKwqBOEBa1gjCoEzSr1W0PzrmTqnqTiDwnp66o/L1zbntkI0PVoFYQBnWCsKgVhEGdoCWaW9qjPAejl6aqOOc0jv1SJ1Vnk3OuLo4dUyvVhXMKQuKcglBaOqdwhzcAAABkBpNfAAAAZAaTXwAAAGQGk18AAABkBpNfAAAAZAaTXwAAAGQGk18AAABkBpNfAAAAZAaTXwAAAGRGq29vDCBaw4YNM3nt2rUFX7Nu3TqTZ82alff7AJJp+PDhgW1XXnll3tdcc801Jvfs2dPko0ePmvzII4+Y/NhjjwX2uXPnTpMPHTqUdwxAGvHJLwAAADKDyS8AAAAyg8kvAAAAMkOdc+U7mGr5DtaCyy+/3ORf/epXJtfV1QVe88EHH8Q6pk6dOgW2zZ8/3+SxY8ea3L17d5P3798f/cAKcM5pHPtNQp3EYebMmSbPmDEj9mOqxvIWFWuTcy74H1YEqrVWsirL55TJkycHtl177bV5XzNo0CCTO3fubLL/33+Yv+9ff/11k7/xjW+YvG/fvoL7KAPOKZ6amhqTt23bZvKqVatMLlQLfv/4iBEjAs85duyYyQcOHDDZv+bkT3/6k8nLli3LO4YotHRO4ZNfAAAAZAaTXwAAAGQGk18AAABkRtX3/J5zzjkmb9261eRzzz3X5B49egT2EXc/bbdu3QLb3n333byv8cdJz2/yVKLH1+ev++uPqUzozyvRBRdcYLK/vuuAAQNMHjVqVGAfixYtMtnv7Xz88cdNfv/994seZ6k4pxSntrbW5I4dO5o8dOhQk7/61a+a7NeRSLBveOPGjSZfccUVJlfi7x7hnBJw1llnmbx8+XKTL7744nIOp1mzZ882+a677or9mPT8AgAAIPOY/AIAACAzmPwCAAAgM9pVegBx89fo3bJli8l+z29SrVy50uSDBw9WaCRoybBhw0wutsfXXxNx/fr1BV9TiT5iRM8/Dy1evNjk5tbYzOfkyZOBbRMnTjTZX1fW7xmcO3euyeW8PqQa+D2WTz/9tMmPPfaYybfccktgHydOnMh7jH/84x95v++v9eqbMmVKYNuePXtM9te+9495/vnnm/zWW2/lPSbiceTIEZP9v4/q6+tN9teIPv30001+5ZVXCh5z+vTpJvvXGhw/ftzkFStWFNxnufDJLwAAADKDyS8AAAAyg8kvAAAAMqPqe3793re9e/dWaCT/r0OHDib7vV/N8fvvmuvpQ2WtXbu2qOdHsQZvoZ5fv48YyXD11VebvHTpUpPbtct/avbXXt25c6fJv/jFLwKv8deA9XtQf/7zn+fd55NPPpl3TLDGjBljst9T/b3vfc/k3/72t4F9bN++PfqBNfGvf/0rsO2ee+4x+b777jPZ7w31+z6///3vRzQ6RMnv4Q3T09vUkCFDAtv8PmKf31Ne7DHjxCe/AAAAyAwmvwAAAMgMJr8AAADIjKrv+fW9/fbbJr/88ssm+2vlxcG/v3qYe27T45t+fv9tsT2+rekJpue38gYMGBDYtmTJEpP3799v8re//W2Tzz77bJOff/55kz/++OOixzV16lST58+fb3Lfvn2L3ifC8/t54+7vDctfY9pff/hzn/ucyePGjTN5zpw5Ju/atSvC0SEubdrYz0JHjx5t8qOPPhp4jX9e8q97CXM9U6XwyS8AAAAyg8kvAAAAMqPg5FdVf6+q+1V1W5NtNaq6SlV35P7sHO8wkQbUCsKgThAWtYIwqBMUK0zP70IReUBEmjYCTRORNc65uao6LZenNvPaihs4cKDJ/fv3N9m/D/mxY8diH9OXv/xlk5tb0zOp/WAFLJQU10rchg8fXtTz/XuzF1rTtzXHqJCFUsV1UldXZ/KLL74YeI6/Vqq/XuaWLVuiH5jn3HPPjf0YEVgoKamV2tpak8ePH5/3+a+99lqMo2m9w4cPmzxp0iSTV69ebbJfy/55q0w9vwslJXWSFBdddJHJ/rrTP/jBD0w+ePBgYB/z5s0zefbs2SZ/+OGHpQwxVgU/+XXOvSAi/k99pYgsyn29SESuinhcSCFqBWFQJwiLWkEY1AmK1dqe327OuU9vlfaeiHSLaDyoPtQKwqBOEBa1gjCoE7So5KXOnHNOVV1L31fVG0TkhlKPg/TLVyvUCT7FOQVhcU5BGJxT4GvtJ7/7VLWHiEjuz/0tPdE51+Ccq3PO1bX0HFS1ULVCnWQe5xSExTkFYXBOQYta+8nvchGZICJzc38+HdmIIubf1GLMmDEm+xeb+Qt4i4js3bvX5BMnTpQ0ptNOO63gc/zFoj/44IOSjllBqamVuPkXghR7AwpVjW4wyZPaOunWzf5rqn+ziOYuaF2/fr3JjY2N0Q+sgOuuuy7v9/0bcSRIImtl8ODBJn/2s5/N+/wNGzbEOZzI+Bdbv/nmmyb37t3b5BEjRpi8YMGCeAZWWCLrpFL8m2s98sgjJvfr1y/v6++9997Atvvvv9/kctwkLCphljr7g4hsEJEvqOrbqjpZThXTpaq6Q0RG5jIyjlpBGNQJwqJWEAZ1gmIV/OTXOffdFr41ooXtyChqBWFQJwiLWkEY1AmKxR3eAAAAkBklr/aQdAcOHDD5mWeeMfk73/mOyX4/k0hwkfpnn33W5K5du+bdh3P2ItO7777b5KNHjwaO+etf/zqwDcnm32DC79v2s/98vwe42J5gkdL7ilE8/yYA/o11/P5ekeD7VA7du3c3uUuXLia//PLLJu/bty/2MVWTQj3Ux48fN9m/HiWp9u+314k1NDSYPGfOHJP962q+9a1vmbxs2bIIR4eWfPOb3zR56dKlJrdv397kzZs3mzxu3DiTm7suwZ/bpAmf/AIAACAzmPwCAAAgM5j8AgAAIDOqvufXt2vXrqJfM2TIkLzZ16aN/X+KTz75JO/z/Z4qkdaNE5VVqGfX7/OcMWNG3v0V+/zmFOorRukmTJhgsr8m96xZs8o5HBERadu2bWDbrbfearLf8ztv3jyT/R5VWP6a8Jdcckne5x86dMhk/1qStHjooYdMvu2220yuqakx2T9vrV69OrDPw4cPRzS6bOjcuXNg24033mjynXfeabLf4+uvyeuv4/vWW2+ZnOb+3ubwyS8AAAAyg8kvAAAAMoPJLwAAADIjcz2//n3Gv/a1r5k8atSoko9Rbb0xaJ1CvZ5+T28l1n5F6f785z+b/Pzzz5vsr+9cDoMGDQps+8lPfmLy4sWLTX7hhRdiHVO1adfO/vV55pln5n2+qsY5nLLxe9r9a1r8a1569+5t8llnnRXYJz2/+Y0YYW9U589jRII96D6//vz3YdGiRSb7vdwbNmwI7NPvM04TPvkFAABAZjD5BQAAQGYw+QUAAEBmaDn7U1U18c2wgwcPDmy78MIL875m4MCBJo8cOdLknj17muz/zptb57d79+55j5kEzrlYmtjSUCetEcd/a35f8cyZMyM/RgQ2Oefq4thxtdZKserr601euXJl4Dl+j2r//v1Nfu+996IfWJHSdE7xe3zXrFlj8te//nWTN23aZHJzf9ek0fz5802eNGmSyX5P/FVXXRXFYTN1Tpk8ebLJDz/8cOA5J06cMPmll17Km/3rAs4444y8Y7j44osD2xobG00eP368yZs3b867z3Jo6ZzCJ78AAADIDCa/AAAAyAwmvwAAAMiMzK3zW8hf//rXUNvy6dOnj8n+up+9evUyefny5UXtH8nk99v697Qv1fDhwwPb1q1bF+kxkA5f+cpXTF6xYoXJza05O2bMGJOT0OObZh9++KHJ+/bty/v8Dh06xDmcitm9e3elh1D1/HV9Dx06FHjOtm3bTPb7cUv1m9/8JrDNX+f3xz/+scl+/3eS8MkvAAAAMoPJLwAAADKDyS8AAAAyg57fMih0z/c333yzTCNBVJpbT3fo0KGxHnPYsGGBbfT8ZkPHjh1NnjNnjsk1NTUmNzQ0BPaxatWq6AeG//PRRx/l/X5tba3Jft/2li1boh4SqtSyZcvKfszjx48XfM7pp59ehpFEg09+AQAAkBlMfgEAAJAZTH4BAACQGUx+AQAAkBlc8FYG/sUoSJ+1a9ea3NzFZ8Xyb1rh7zPqm2Qgve6++26TR48ebbJ/gZu/2Dzi97vf/c7ksWPHmnzGGWeYfMkll5ic1gveevToUekhoAyuu+66gs95/PHHyzCSaPDJLwAAADKDyS8AAAAyg8kvAAAAMoOe3xhcfvnllR4CShRFj69/Awq/x9dHjy8+df3115t88803m/zss8+afOONN8Y+JuS3ceNGk3fv3m1ynz59TPb7uPfs2RPY5x//+MeIRhcfv7fZ19jYWKaRoBRt2tjPQp988kmTzznnnMBrDh48aHKabqTDJ78AAADIDCa/AAAAyIyCk19V7aWqa1X176q6XVWn5LbXqOoqVd2R+7Nz/MNFUlEnCItaQVjUCsKgTlCsMD2/J0XkVufc31S1k4hsUtVVIjJRRNY45+aq6jQRmSYiU+MbanpcccUVRT3/iSeeiGkkZZXqOnHOlbwPv6fX7/n1xbF2cEqkulbiUF9fb/KCBQtMPnr0qMmzZs2KfUwJkZpaOXz4sMmTJ0822e+HPPPMM02ePn16YJ/9+vUzed68eaUMMRRVNblv374m/+xnPzO5a9euJm/dutXku+66K8LRtShRddK/f3+T/fVv/R79119/3eRPPvkknoE10bZtW5Nnzpxp8tVXX23yyZMnA/uYPXu2ycePH49mcGVQ8JNf59xe59zfcl8fFZE3ROQ8EblSRBblnrZIRK6Ka5BIPuoEYVErCItaQRjUCYpV1GoPqlorIoNE5FUR6eac25v71nsi0q2F19wgIje0fohIG+oEYVErCKvYWqFOsolzCsIIfcGbqnYUkWUicrNz7kjT77lT/2bc7L8bO+canHN1zrm6kkaKVKBOEBa1grBaUyvUSfZwTkFYGqbXUVVPE5FnROQ559wvc9saRWSYc26vqvYQkXXOuS8U2E/pjZUp4PfrFPodd+rUKbDto48+inRMcXDOmeawNNdJsT2/zfXzrl+/3uShQ4eaXGpPr9+LlyKb/L9U0lwrUTjvvPNM3rFjh8kdOnQwefTo0Sb/5S9/iWdgFeafU0SiqZUk1MkPf/hDk/1+ybPPPjvwGr/PcvHixUUd88gRM/+T1atXm3zNNdcEXvOZz3zG5HHjxuU9xrvvvmvyyJEjTY5pnd9En1P86z/WrFmT9/lTp9o25Dh6u7t1sx9633HHHSb/6Ec/yvv6KVOmBLbdf//9pQ8sZs2dU0TCrfagIrJARN74tKBylovIhNzXE0Tk6VIHifSiThAWtYKwqBWEQZ2gWGF6fi8SkfEislVVN+e23SEic0VkqapOFpE9IpL/Ni+odtQJwqJWEBa1gjCoExSl4OTXOfeiiLT0760joh0O0oo6QVjUCsKiVhAGdYJiher5jexgCei7Kodp06aZ7Pd2+aql5zcqlagTvx/XX4O3HPw+Yr9vLMUC/XlRScM5ZcCAAYFtr776qskdO3Y0+ZZbbjH5vvvuM7mc5+1yqqZzSiFf+tKXTG6u53LChAkmt2/fvqhj+NcJtKZu3n//fZP9Nagffvhhk3fv3l30MVoh0eeUNm1sR+ntt99u8j333GOyf53QxIkTTV6xYoXJ/nsiItKzZ0+T/XWmx461H3p/8YtfNPmNN94w+ac//anJK1euDBwzDeehVvf8AgAAANWCyS8AAAAyg8kvAAAAMoOe3xi89NJLJtfX15u8fft2k+vqgq1LabhHdjX35/k9wDNmzCj6NbNmzcr7fL/Ht7m1g6tEovvzouav4euvrSoicv7555s8f/58k2+77TaTDx8+HNHokq2azymt0bdvX5NvuummvM8fOHCgyZs3b877/SVLlgT24feONjQ0mOyv61shqTqn+D3A/hrwTzzxhMldu3Y1+Z133jG5uWuCampqTO7SpYvJfv+33+M7adIkk1955ZXAMdKInl8AAABkHpNfAAAAZAaTXwAAAGQGPb8x8Nfs83/Hfn/P9ddfX3AfSUR/HkJKVX9esS699FKTH3zwQZNra2sDr7n33ntNnj59uskff/xxNINLGc4pCKmqzin9+vUz+YEHHjD5sssuK3qfzz33nMlPPfWUyY8++qjJx44dK/oYaUDPLwAAADKPyS8AAAAyg8kvAAAAMoPJLwAAADKDC97QalycgpCq6uIUn78Y/AUXXGByczcm8C+KwymcUxBSVZ9TEB0ueAMAAEDmMfkFAABAZjD5BQAAQGa0q/QAACDN6uvrKz0EAEAR+OQXAAAAmcHkFwAAAJnB5BcAAACZweQXAAAAmcHkFwAAAJnB5BcAAACZweQXAAAAmVHudX4PiMgeEema+zrJGGN+/xbjvtNUJyLpGCe1UnmMMb9y1IkI70NUqr1WeA+iU6lxtlgn6pwr50BOHVR1o3OuruwHLgJjrLy0/HxpGGcaxliKNPx8jDEZ0vAzMsbKS8PPl4YxiiRznLQ9AAAAIDOY/AIAACAzKjX5bajQcYvBGCsvLT9fGsaZhjGWIg0/H2NMhjT8jIyx8tLw86VhjCIJHGdFen4BAACASqDtAQAAAJnB5BcAAACZUdbJr6qOUtVGVd2pqtPKeex8VPX3qrpfVbc12VajqqtUdUfuz84VHmMvVV2rqn9X1e2qOiWJ44xKEmuFOkmeJNaJCLWSRNRKq8eXqToRSWatJL1OcuNJTa2UbfKrqm1F5EER+XcRGSAi31XVAeU6fgELRWSUt22aiKxxzn1eRNbkciWdFJFbnXMDRKReRP4j9/tL2jhLluBaWSjUSWIkuE5EqJVEoVZKkpk6EUl0rSyUZNeJSJpqxTlXloeIXCgizzXJt4vI7eU6fojx1YrItia5UUR65L7uISKNlR6jN96nReTSpI+z2mqFOknOI8l1Qq0k60GtUCfVUCtpqpOk10o52x7OE5F/Nslv57YlVTfn3N7c1++JSLdKDqYpVa0VkUEi8qokeJwlSFOtJPb3T50kTmLfA2olcRL5HmSgTkTSVSuJfQ+SXitc8BaCO/W/K4lYE05VO4rIMhG52Tl3pOn3kjTOLErS7586SbYkvQfUSrIl5T2gTpItSe9BGmqlnJPfd0SkV5PcM7ctqfapag8Rkdyf+ys8HlHV0+RUQS1xzj2V25y4cUYgTbWSuN8/dZJYiXsPqJXEStR7kKE6EUlXrSTuPUhLrZRz8vuaiHxeVXuransRuVZElpfx+MVaLiITcl9PkFO9KxWjqioiC0TkDefcL5t8K1HjjEiaaiVRv3/qJLF1IpKw94BaoVbCyFidiKSrVhL1HqSqVsrc/DxaRP5bRHaJyJ2VbnhuMq4/iMheETkhp/p7JotIFzl1VeIOEVktIjUVHuMQOfVPBVtEZHPuMTpp46zmWqFOkvdIYp1QK8l8UCvUSZprJel1krZa4fbGAAAAyAwueAMAAEBmMPkFAABAZjD5BQAAQGYw+QUAAEBmMPkFAABAZjD5BQAAQGYw+QUAAEBm/C9NSElY6Jr9ogAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x864 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_mnist(mnist_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a non-linear autoencoder which reduces images in the MNIST dataset to a latent representation of 64 dimensions (35 marks)\n",
    "\n",
    "Using the Keras library, design a non-linear autoencoder that reduces the original MNIST images in the ```mnist_train_x.npy``` file to a dimension of 64 before reconstructing the original images. Use your ```display_mnist()``` function to display the first 5 reconstructed images from the ```mnist_train_x.npy``` dataset.\n",
    "#### Requirements\n",
    "1. The autoencoder must use only ```Dense``` layers.\n",
    "2. The autoencoder must use 4 layer in total, 3 hidden layers and 1 output layer.\n",
    "3. The autoencoder must be trained for 10 epochs.\n",
    "4. Use the 'relu' activation function for each hidden layer. This activation function is an alternative to the traditional Sigmoid function and can help a neural network converge. This  'relu' activation function  will be introduced in the lectures in Week 7. \n",
    "5. Use Stochastic Gradient Decent (SGD) as the optimization method.\n",
    "6. Both inputs and outputs should be scaled to the range 0 to 1. Use an appropriate activation function for the output layer.\n",
    "7. Use the the 'binary_crossentropy' loss function, which is a loss function for values scaled to the range 0 to 1.\n",
    "8. Use a batch size of 16.\n",
    "9. The output layer should reconstruct the original images of dimension 784.\n",
    "9. Choose a suitable value for learning rate and number of neurons for the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.1717 - accuracy: 0.8012\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.1113 - accuracy: 0.8121\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.0994 - accuracy: 0.8134\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0936 - accuracy: 0.8139\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.0897 - accuracy: 0.8142\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0870 - accuracy: 0.8144\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s 192us/step - loss: 0.0850 - accuracy: 0.8146\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0836 - accuracy: 0.8147\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.0825 - accuracy: 0.8147\n",
      "Epoch 10/10\n",
      "26240/60000 [============>.................] - ETA: 5s - loss: 0.0819 - accuracy: 0.8149"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d7dc15853f14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/local/java/python-pip-packages.cs342/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train = np.reshape(mnist_train_x, (60000,784))/255\n",
    "\n",
    "learning = [1]\n",
    "\n",
    "for i in learning:\n",
    "    \n",
    "    sgd = SGD(learning_rate=i)\n",
    "    autoencoder = Sequential()\n",
    "\n",
    "    autoencoder.add(Dense(128, use_bias='true', activation='relu', input_dim = 784))\n",
    "    autoencoder.add(Dense(64, use_bias='true', activation='relu'))\n",
    "    autoencoder.add(Dense(128, use_bias='true', activation='relu'))\n",
    "    autoencoder.add(Dense(784, use_bias='true', activation='sigmoid'))\n",
    "\n",
    "    autoencoder.compile(loss='binary_crossentropy', optimizer=sgd , metrics=['accuracy'])\n",
    "\n",
    "    autoencoder.fit(x_train, x_train, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "    img = autoencoder.predict(x_train[:5])\n",
    "\n",
    "\n",
    "display_mnist(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explain how modifying the learning rate impacts results (10 marks)\n",
    "\n",
    "In a Jupyter markdown block (i.e. text block), explain how modifying the learning rate of SGD may change the results obtained in question 2. Why is it necessary to choose an appropriate learning rate in SGD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise 3 discussion_\n",
    "\n",
    "_We can notice that increasing the learning rate makes the images clearer. This is because the learning rate influences the speed of convergence of the neural network.\n",
    "A high value will make it converge more quickly than a smaller value._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create an autoencoder which reduces images in the MNIST dataset to a latent representation of 1 dimension (5 marks)\n",
    "Modify the non-linear autoencoder in question 2 to compute latent representations of 1 dimension. Use the ```display_mnist()``` to display the first 5 reconstructed images from the ```mnist_train_x.npy``` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in learning:\n",
    "    \n",
    "    sgd = SGD(learning_rate=i)\n",
    "    autoencoder = Sequential()\n",
    "\n",
    "    autoencoder.add(Dense(64, use_bias='true', activation='relu', input_dim = 784))\n",
    "    autoencoder.add(Dense(1, use_bias='true', activation='relu'))\n",
    "    autoencoder.add(Dense(64, use_bias='true', activation='relu'))\n",
    "    autoencoder.add(Dense(784, use_bias='true', activation='sigmoid'))\n",
    "\n",
    "    autoencoder.compile(loss='binary_crossentropy', optimizer=sgd , metrics=['accuracy'])\n",
    "\n",
    "    autoencoder.fit(x_train, x_train, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "    score = autoencoder.evaluate(x_train, x_train, batch_size=16)\n",
    "    print (score)\n",
    "\n",
    "    img = autoencoder.predict(x_train[:5])\n",
    "\n",
    "\n",
    "    n = 5  # how many digits we will display\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(n):\n",
    "    # display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(img[i].reshape(28, 28), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Contrast your results in questions 4 and 2. Discuss any differences you observe (5 marks)\n",
    "How do the reconstructed images in question 2 compare to those reconstructed by using the non-linear autoencoder in question 4? In a Jupyter markdown block (i.e. text block), please explain your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise 5 discussion_\n",
    "\n",
    "_The fact that the NN is reduce to a 1 neuron layer is not optimal as a lot of information is lost there, as the results show. This because all precedent neurons are given the same information in the next layer._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Denoising autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will create a non-linear denoising autoencoder. \n",
    "Use your ```display_mnist()``` function to display the first 5 images from the ```mnist_test_x.npy``` dataset. You will notice that these images have been corrupted with noise. We will use a non-linear denoising autoencoder to attempt to remove the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_x = np.load(data_path + '/mnist_test_x.npy')\n",
    "print('MNIST test set shape: ', mnist_test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the noise form the test dataset ```mnist_test_x.npy```, we can use supervised learning. If we have a noisy image and a corresponding clean image, we can train a supervised learning model to predict the clean image, given the noisy image. However, we do not have any clean image for our test set, so how will we train a model? Fortunately, you know that the noisy images have been generated by adding Gaussian noise with a mean of 127 and a variance of 500. This means we can generate new training data by adding noise to the clean training images. This way, we have a correspondence between clean and noisy images, which we can use to train a supervised learning model. Once the model is trained, we can use it to de-noise the noisy test dataset, for which we do not have the ground truth. \n",
    "\n",
    "To summarize, you should proceed as follows:\n",
    "\n",
    "\n",
    "1. Add Gaussian noise to the clean training data to generate noisy training data.\n",
    "2. Train a supervised learning model using the generated noisy training data as input and clean training data as the targets.\n",
    "3. Run the trained model using the noisy test data as inputs to recover the clean test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Add Gaussian noise to the training images (10 marks)\n",
    "\n",
    "Create a function to add Gaussian noise with mean 127 and variance 500 to each image in the original ```mnist_train_x.npy``` dataset. Use the ```display_mnist()``` function to display the first 5 images with added noise, and verify that they appear similar to the images in the ```mnist_test_x.npy``` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def Add_Gnoise(data):\n",
    "    \n",
    "\n",
    "    mean = 127\n",
    "    var = 500\n",
    "    sigma = var**0.5\n",
    "\n",
    "    row,col,ch= data.shape\n",
    "    gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
    "    gauss = gauss.reshape(row,col,ch)\n",
    "    noisy_imgs = data + gauss\n",
    "    return noisy_imgs\n",
    "\n",
    "    \n",
    "\n",
    "display_mnist(Add_Gnoise(mnist_train_x))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train a non-linear denoising autoencoder using the generated noisy train images (25 marks)\n",
    "\n",
    "Using the same autoencoder architecture as in question 2 (with a latent representation of 64 dimensions), train a denoising autoencoder to take a noisy image as input and output a denoised version of the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_imgs = np.reshape(Add_Gnoise(mnist_train_x),(60000,784))/255\n",
    "\n",
    "sgd = SGD(learning_rate=1)\n",
    "autoencoder2 = Sequential()\n",
    "\n",
    "autoencoder2.add(Dense(128, use_bias='true', activation='relu', input_dim = 784))\n",
    "autoencoder2.add(Dense(64, use_bias='true', activation='relu'))\n",
    "autoencoder2.add(Dense(128, use_bias='true', activation='relu'))\n",
    "autoencoder2.add(Dense(784, use_bias='true', activation='sigmoid'))\n",
    "\n",
    "autoencoder2.compile(loss='binary_crossentropy', optimizer=sgd , metrics=['accuracy'])\n",
    "\n",
    "autoencoder2.fit(noisy_imgs, x_train, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "score = autoencoder2.evaluate(noisy_imgs, x_train, batch_size=16)\n",
    "print (score)\n",
    "\n",
    "img = autoencoder2.predict(noisy_imgs[:5])\n",
    "\n",
    "\n",
    "n = 5  # how many digits we will display\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(img[i].reshape(28, 28),cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Denoise the images in the test set (5 marks)\n",
    "\n",
    "Use the trained model to de-noise the images in the test set (```mnist_test_x.npy```) and display the first 5 images with the ```display_mnist()``` function. Save the entire denoised dataset in a file called ```mnist_test_x_denoised.npy```. We will use these images in lab 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.reshape(mnist_test_x, (5000,784))/255\n",
    "\n",
    "\n",
    "sgd = SGD(learning_rate=1)\n",
    "autoencoder1 = Sequential()\n",
    "\n",
    "autoencoder1.add(Dense(128, use_bias='true', activation='relu', input_dim = 784))\n",
    "autoencoder1.add(Dense(64, use_bias='true', activation='relu'))\n",
    "autoencoder1.add(Dense(128, use_bias='true', activation='relu'))\n",
    "autoencoder1.add(Dense(784, use_bias='true', activation='sigmoid'))\n",
    "\n",
    "autoencoder1.compile(loss='binary_crossentropy', optimizer=sgd , metrics=['accuracy'])\n",
    "\n",
    "autoencoder1.fit(noisy_imgs, x_train, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "mnist_test_x_denoised = autoencoder1.predict(x_test)\n",
    "\n",
    "np.save('mnist_test_x_denoised.npy', mnist_test_x_denoised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_mnist(mnist_test_x_denoised[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
